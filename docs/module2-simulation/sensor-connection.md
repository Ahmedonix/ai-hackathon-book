# Connecting Gazebo Sensor Data to ROS 2 Nodes

## Overview

In this section, we'll explore how to properly connect sensor data generated by Gazebo to ROS 2 nodes. This critical step enables your humanoid robot to perceive its environment in simulation, allowing for implementation of navigation, perception, and control algorithms.

## Understanding Gazebo-ROS 2 Sensor Communication

### 1. Sensor Plugin Architecture

Gazebo uses plugins to generate sensor data and publish it to ROS 2 topics:

- **Sensor Plugins**: Generate raw sensor data based on physics simulation
- **ROS Interface**: Publish sensor data using ROS 2 message types
- **Message Types**: Follow standard ROS 2 sensor message conventions

### 2. Standard Sensor Message Types

Gazebo sensor plugins publish the following standard ROS 2 message types:

- `sensor_msgs/LaserScan`: For LiDAR and range sensors
- `sensor_msgs/Image`: For camera sensors
- `sensor_msgs/CameraInfo`: Camera calibration data
- `sensor_msgs/Imu`: For inertial measurement units
- `sensor_msgs/JointState`: For robot joint positions

## Implementing Sensor Connections

### 1. LiDAR Sensor Connection

Here's how to properly connect a LiDAR sensor from Gazebo to ROS 2:

```xml
<!-- In your URDF/robot definition -->
<gazebo reference="lidar_link">
  <sensor name="laser" type="ray">
    <always_on>true</always_on>
    <visualize>false</visualize>
    <update_rate>10</update_rate>
    <ray>
      <scan>
        <horizontal>
          <samples>720</samples>
          <resolution>1</resolution>
          <min_angle>-3.14159</min_angle> <!-- -π radians -->
          <max_angle>3.14159</max_angle>   <!-- π radians -->
        </horizontal>
      </scan>
      <range>
        <min>0.1</min>
        <max>30.0</max>
        <resolution>0.01</resolution>
      </range>
    </ray>
    <plugin name="gazebo_ros_laser" filename="libgazebo_ros_ray_sensor.so">
      <ros>
        <namespace>/laser</namespace>
        <argument>~/out:=scan</argument>
      </ros>
      <output_type>sensor_msgs/LaserScan</output_type>
      <frame_name>lidar_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

### 2. Camera Sensor Connection

Configuration for connecting camera sensors:

```xml
<gazebo reference="camera_link">
  <sensor name="camera" type="camera">
    <always_on>true</always_on>
    <visualize>true</visualize>
    <update_rate>30</update_rate>
    <camera name="head_camera">
      <horizontal_fov>1.047</horizontal_fov> <!-- 60 degrees -->
      <image>
        <width>640</width>
        <height>480</height>
        <format>R8G8B8</format>
      </image>
      <clip>
        <near>0.1</near>
        <far>100</far>
      </clip>
    </camera>
    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">
      <ros>
        <namespace>/camera</namespace>
        <argument>~/image_raw:=image_raw</argument>
        <argument>~/camera_info:=camera_info</argument>
      </ros>
      <camera_name>camera</camera_name>
      <frame_name>camera_link</frame_name>
    </plugin>
  </sensor>
</gazebo>
```

### 3. IMU Sensor Connection

For IMU sensors that provide orientation and acceleration data:

```xml
<gazebo reference="imu_link">
  <sensor name="imu_sensor" type="imu">
    <always_on>true</always_on>
    <update_rate>100</update_rate>
    <visualize>false</visualize>
    <plugin name="gazebo_ros_imu" filename="libgazebo_ros_imu.so">
      <ros>
        <namespace>/imu</namespace>
        <argument>~/out:=data</argument>
      </ros>
      <frame_name>imu_link</frame_name>
      <body_name>base_link</body_name>
    </plugin>
  </sensor>
</gazebo>
```

## Processing Sensor Data in ROS 2 Nodes

### 1. LiDAR Data Processing Node

Create a ROS 2 node to process LiDAR data from Gazebo:

```python
# scripts/lidar_processor.py
#!/usr/bin/env python3

"""
LiDAR processor node for processing Gazebo sensor data in ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
from std_msgs.msg import Float32MultiArray
from visualization_msgs.msg import MarkerArray, Marker
import numpy as np
import math


class LiDARProcessor(Node):
    def __init__(self):
        super().__init__('lidar_processor')
        
        # Subscribe to LiDAR data from Gazebo
        self.lidar_subscription = self.create_subscription(
            LaserScan,
            '/laser/scan',  # Topic where Gazebo publishes LiDAR data
            self.lidar_callback,
            10
        )
        
        # Publisher for processed LiDAR data
        self.processed_publisher = self.create_publisher(
            Float32MultiArray,
            '/processed_lidar_data',
            10
        )
        
        # Publisher for visualization markers
        self.visualization_publisher = self.create_publisher(
            MarkerArray,
            '/lidar_visualization',
            10
        )
        
        # Publisher for obstacle detection
        self.obstacle_publisher = self.create_publisher(
            Float32MultiArray,
            '/obstacles',
            10
        )
        
        # Parameters
        self.min_obstacle_distance = 1.0  # meters
        self.front_angle_range = 30  # degrees to check in front
        
        self.get_logger().info('LiDAR Processor Node Initialized')

    def lidar_callback(self, msg):
        """Process incoming LiDAR data from Gazebo"""
        self.get_logger().debug(f'Received LiDAR scan with {len(msg.ranges)} points')
        
        # Process the scan data
        ranges = np.array(msg.ranges)
        
        # Filter out invalid readings (inf, nan)
        valid_ranges = ranges[np.isfinite(ranges)]
        
        if len(valid_ranges) > 0:
            # Calculate minimum distance
            min_distance = np.min(valid_ranges)
            self.get_logger().debug(f'Min distance: {min_distance:.2f}m')
            
            # Detect obstacles in front of robot
            obstacle_data = self.detect_front_obstacles(msg)
            if obstacle_data:
                obstacle_msg = Float32MultiArray()
                obstacle_msg.data = obstacle_data
                self.obstacle_publisher.publish(obstacle_msg)
            
            # Process and publish simplified data
            processed_data = Float32MultiArray()
            processed_data.data = [
                float(min_distance),
                float(np.mean(valid_ranges)),
                float(len(valid_ranges)),
                float(msg.angle_min),
                float(msg.angle_max)
            ]
            self.processed_publisher.publish(processed_data)
            
            # Publish visualization markers
            self.publish_visualization(msg)
        else:
            self.get_logger().warn('No valid LiDAR readings')

    def detect_front_obstacles(self, scan_msg):
        """Detect obstacles directly in front of the robot"""
        # Calculate angle indices for the front sector
        angle_increment = scan_msg.angle_increment
        front_center_idx = int(len(scan_msg.ranges) / 2)
        
        # Calculate how many points correspond to our front angle range
        angle_range_rad = math.radians(self.front_angle_range)
        points_per_side = int(angle_range_rad / (2 * angle_increment))
        
        front_start = max(0, front_center_idx - points_per_side)
        front_end = min(len(scan_msg.ranges), front_center_idx + points_per_side)
        
        # Get distances in the front sector
        front_ranges = scan_msg.ranges[front_start:front_end]
        valid_front_ranges = [r for r in front_ranges if r <= self.min_obstacle_distance and np.isfinite(r)]
        
        if valid_front_ranges:
            min_front_distance = min(valid_front_ranges)
            return [min_front_distance, float(front_start), float(front_end)]
        
        return []  # No obstacles detected

    def publish_visualization(self, scan_msg):
        """Publish visualization markers for the LiDAR data"""
        marker_array = MarkerArray()
        
        # Create markers for detected obstacles
        obstacle_data = self.detect_front_obstacles(scan_msg)
        if obstacle_data:
            obstacle_marker = Marker()
            obstacle_marker.header = scan_msg.header
            obstacle_marker.ns = "lidar_obstacles"
            obstacle_marker.id = 0
            obstacle_marker.type = Marker.CUBE
            obstacle_marker.action = Marker.ADD
            
            # Position based on min distance in front
            obstacle_marker.pose.position.x = obstacle_data[0]  # distance forward
            obstacle_marker.pose.position.y = 0.0
            obstacle_marker.pose.position.z = 0.5  # height
            
            obstacle_marker.pose.orientation.w = 1.0
            obstacle_marker.scale.x = 0.5  # width
            obstacle_marker.scale.y = 0.5  # depth
            obstacle_marker.scale.z = 1.0  # height
            
            obstacle_marker.color.a = 0.8  # alpha (transparency)
            obstacle_marker.color.r = 1.0  # red
            obstacle_marker.color.g = 0.0
            obstacle_marker.color.b = 0.0
            
            marker_array.markers.append(obstacle_marker)
        
        # Publish all markers
        self.visualization_publisher.publish(marker_array)

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info('LiDAR Processor Node Shutting Down')
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    processor = LiDARProcessor()
    
    try:
        rclpy.spin(processor)
    except KeyboardInterrupt:
        processor.get_logger().info('Node interrupted by user')
    finally:
        processor.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### 2. Camera Data Processing Node

Create a node for processing camera data from Gazebo:

```python
# scripts/camera_processor.py
#!/usr/bin/env python3

"""
Camera processor node for processing Gazebo camera data in ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Image, CameraInfo
from cv_bridge import CvBridge
import cv2
import numpy as np


class CameraProcessor(Node):
    def __init__(self):
        super().__init__('camera_processor')
        
        # Initialize CV Bridge for OpenCV integration
        self.bridge = CvBridge()
        
        # Subscribe to camera data from Gazebo
        self.image_subscription = self.create_subscription(
            Image,
            '/camera/image_raw',  # Topic where Gazebo publishes camera images
            self.image_callback,
            10
        )
        
        # Subscribe to camera info
        self.info_subscription = self.create_subscription(
            CameraInfo,
            '/camera/camera_info',
            self.info_callback,
            10
        )
        
        # Publisher for processed image
        self.processed_image_publisher = self.create_publisher(
            Image,
            '/processed_image',
            10
        )
        
        # Publisher for object detection results
        self.detection_publisher = self.create_publisher(
            Image,
            '/detection_result',
            10
        )
        
        # Internal state
        self.camera_info = None
        self.processing_enabled = True
        
        self.get_logger().info('Camera Processor Node Initialized')

    def info_callback(self, msg):
        """Store camera info for use in processing"""
        self.camera_info = msg

    def image_callback(self, msg):
        """Process incoming camera image from Gazebo"""
        if not self.processing_enabled:
            return
            
        try:
            # Convert ROS Image message to OpenCV image
            cv_image = self.bridge.imgmsg_to_cv2(msg, desired_encoding='bgr8')
            
            # Process the image (example: apply a simple filter)
            processed_image = self.process_image(cv_image)
            
            # Convert back to ROS Image message
            processed_msg = self.bridge.cv2_to_imgmsg(processed_image, encoding='bgr8')
            processed_msg.header = msg.header  # Preserve timestamp and frame ID
            
            # Publish processed image
            self.processed_image_publisher.publish(processed_msg)
            
            # Perform simple object detection (example)
            detection_result = self.detect_objects(cv_image)
            if detection_result is not None:
                detection_msg = self.bridge.cv2_to_imgmsg(detection_result, encoding='bgr8')
                detection_msg.header = msg.header
                self.detection_publisher.publish(detection_msg)
                
        except Exception as e:
            self.get_logger().error(f'Error processing image: {str(e)}')

    def process_image(self, cv_image):
        """Apply basic image processing"""
        # Example: Convert to gray and apply Gaussian blur
        gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        # Convert back to BGR for publishing
        processed = cv2.cvtColor(blurred, cv2.COLOR_GRAY2BGR)
        return processed

    def detect_objects(self, cv_image):
        """Simple object detection example"""
        try:
            # Create a copy of the image to draw on
            result_image = cv_image.copy()
            
            # Convert to HSV for color-based detection
            hsv = cv2.cvtColor(cv_image, cv2.COLOR_BGR2HSV)
            
            # Define range for detecting red color (as example)
            lower_red = np.array([0, 50, 50])
            upper_red = np.array([10, 255, 255])
            mask1 = cv2.inRange(hsv, lower_red, upper_red)
            
            lower_red = np.array([170, 50, 50])
            upper_red = np.array([180, 255, 255])
            mask2 = cv2.inRange(hsv, lower_red, upper_red)
            
            # Combine masks
            mask = mask1 + mask2
            
            # Find contours
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Draw bounding boxes around detected objects
            for contour in contours:
                area = cv2.contourArea(contour)
                if area > 500:  # Filter out small areas
                    x, y, w, h = cv2.boundingRect(contour)
                    cv2.rectangle(result_image, (x, y), (x+w, y+h), (0, 255, 0), 2)
                    cv2.putText(result_image, f'Red Object ({area:.0f})', (x, y-10), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)
            
            # If we detected anything, return the result image
            if contours:
                return result_image
            else:
                return None
                
        except Exception as e:
            self.get_logger().error(f'Error in object detection: {str(e)}')
            return None

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info('Camera Processor Node Shutting Down')
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    processor = CameraProcessor()
    
    try:
        rclpy.spin(processor)
    except KeyboardInterrupt:
        processor.get_logger().info('Node interrupted by user')
    finally:
        processor.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

### 3. IMU Data Processing Node

Create a node for processing IMU data from Gazebo:

```python
# scripts/imu_processor.py
#!/usr/bin/env python3

"""
IMU processor node for processing Gazebo IMU data in ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import Imu
from std_msgs.msg import Float32MultiArray
from geometry_msgs.msg import Vector3
import math
import numpy as np


class IMUProcessor(Node):
    def __init__(self):
        super().__init__('imu_processor')
        
        # Subscribe to IMU data from Gazebo
        self.imu_subscription = self.create_subscription(
            Imu,
            '/imu/data',  # Topic where Gazebo publishes IMU data
            self.imu_callback,
            10
        )
        
        # Publisher for processed IMU data
        self.attitude_publisher = self.create_publisher(
            Float32MultiArray,
            '/robot_attitude',
            10
        )
        
        # Publisher for balance metrics
        self.balance_publisher = self.create_publisher(
            Float32MultiArray,
            '/balance_metrics',
            10
        )
        
        # Publisher for orientation in Euler angles
        self.euler_publisher = self.create_publisher(
            Vector3,
            '/imu/euler',
            10
        )
        
        # Internal state
        self.last_imu_data = None
        self.processing_enabled = True
        
        # Parameters for humanoid balance
        self.balance_threshold = 0.5  # radians for pitch/roll
        self.upright_tolerance = 0.1  # radians for "upright" check
        
        self.get_logger().info('IMU Processor Node Initialized')

    def imu_callback(self, msg):
        """Process incoming IMU data from Gazebo"""
        if not self.processing_enabled:
            return
            
        self.last_imu_data = msg
        
        # Extract orientation from quaternion
        orientation_q = msg.orientation
        euler_angles = self.quaternion_to_euler(
            orientation_q.x, 
            orientation_q.y, 
            orientation_q.z, 
            orientation_q.w
        )
        
        # Publish Euler angles
        euler_msg = Vector3()
        euler_msg.x = euler_angles[0]  # roll
        euler_msg.y = euler_angles[1]  # pitch
        euler_msg.z = euler_angles[2]  # yaw
        self.euler_publisher.publish(euler_msg)
        
        # Calculate balance metrics
        balance_metrics = self.calculate_balance_metrics(euler_angles, msg.angular_velocity, msg.linear_acceleration)
        
        # Publish balance metrics
        balance_msg = Float32MultiArray()
        balance_msg.data = balance_metrics
        self.balance_publisher.publish(balance_msg)
        
        # Calculate and publish attitude
        attitude_msg = Float32MultiArray()
        attitude_msg.data = [
            euler_angles[0],  # roll
            euler_angles[1],  # pitch
            euler_angles[2],  # yaw
            math.sqrt(msg.angular_velocity.x**2 + msg.angular_velocity.y**2 + msg.angular_velocity.z**2),  # angular velocity magnitude
            math.sqrt(msg.linear_acceleration.x**2 + msg.linear_acceleration.y**2 + msg.linear_acceleration.z**2)  # acceleration magnitude
        ]
        self.attitude_publisher.publish(attitude_msg)
        
        # Log balance status
        if abs(euler_angles[0]) > self.balance_threshold or abs(euler_angles[1]) > self.balance_threshold:
            self.get_logger().warn(f'Balance threshold exceeded! Roll: {euler_angles[0]:.3f}, Pitch: {euler_angles[1]:.3f}')
        elif abs(euler_angles[0]) < self.upright_tolerance and abs(euler_angles[1]) < self.upright_tolerance:
            self.get_logger().info(f'Robot is upright. Roll: {euler_angles[0]:.3f}, Pitch: {euler_angles[1]:.3f}')
        else:
            self.get_logger().info(f'Robot orientation OK. Roll: {euler_angles[0]:.3f}, Pitch: {euler_angles[1]:.3f}')

    def quaternion_to_euler(self, x, y, z, w):
        """Convert quaternion to Euler angles (roll, pitch, yaw)"""
        # Roll (x-axis rotation)
        sinr_cosp = 2 * (w * x + y * z)
        cosr_cosp = 1 - 2 * (x * x + y * y)
        roll = math.atan2(sinr_cosp, cosr_cosp)

        # Pitch (y-axis rotation)
        sinp = 2 * (w * y - z * x)
        if abs(sinp) >= 1:
            pitch = math.copysign(math.pi / 2, sinp)  # Use 90 degrees if out of range
        else:
            pitch = math.asin(sinp)

        # Yaw (z-axis rotation)
        siny_cosp = 2 * (w * z + x * y)
        cosy_cosp = 1 - 2 * (y * y + z * z)
        yaw = math.atan2(siny_cosp, cosy_cosp)

        return roll, pitch, yaw

    def calculate_balance_metrics(self, euler_angles, angular_velocity, linear_acceleration):
        """Calculate balance-related metrics from IMU data"""
        roll, pitch, yaw = euler_angles
        
        # Calculate balance metrics
        roll_stability = abs(roll)  # Lower is more stable
        pitch_stability = abs(pitch)  # Lower is more stable
        angular_velocity_magnitude = math.sqrt(
            angular_velocity.x**2 + 
            angular_velocity.y**2 + 
            angular_velocity.z**2
        )
        
        # Calculate tilt angle from vertical
        tilt_angle = math.sqrt(roll**2 + pitch**2)
        
        # Calculate acceleration metrics (excluding gravity)
        # Note: This is simplified - in practice, you'd need to separate gravity from linear acceleration
        total_acceleration = math.sqrt(
            linear_acceleration.x**2 +
            linear_acceleration.y**2 +
            linear_acceleration.z**2
        )
        
        return [
            float(roll_stability),
            float(pitch_stability),
            float(angular_velocity_magnitude),
            float(tilt_angle),
            float(total_acceleration),
            float(roll),
            float(pitch),
            float(yaw)
        ]

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info('IMU Processor Node Shutting Down')
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    processor = IMUProcessor()
    
    try:
        rclpy.spin(processor)
    except KeyboardInterrupt:
        processor.get_logger().info('Node interrupted by user')
    finally:
        processor.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Sensor Fusion Node

### 1. Multi-Sensor Data Integration

Create a node that integrates data from multiple sensors:

```python
# scripts/sensor_fusion_node.py
#!/usr/bin/env python3

"""
Sensor fusion node for combining data from multiple Gazebo sensors in ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan, Imu, JointState
from nav_msgs.msg import Odometry
from std_msgs.msg import Float32MultiArray
from geometry_msgs.msg import Pose, Twist, Point
import numpy as np
import math


class SensorFusionNode(Node):
    def __init__(self):
        super().__init__('sensor_fusion_node')
        
        # Subscribe to multiple sensor sources
        self.lidar_subscription = self.create_subscription(
            LaserScan,
            '/laser/scan',
            self.lidar_callback,
            10
        )
        
        self.imu_subscription = self.create_subscription(
            Imu,
            '/imu/data',
            self.imu_callback,
            10
        )
        
        self.joint_subscription = self.create_subscription(
            JointState,
            '/joint_states',
            self.joint_callback,
            10
        )
        
        # Publisher for fused sensor data
        self.fused_data_publisher = self.create_publisher(
            Float32MultiArray,
            '/fused_sensor_data',
            10
        )
        
        # Publisher for robot state estimation
        self.state_publisher = self.create_publisher(
            Odometry,
            '/estimated_robot_state',
            10
        )
        
        # Internal state storage
        self.lidar_data = None
        self.imu_data = None
        self.joint_data = None
        self.last_update_time = self.get_clock().now()
        
        # Parameters
        self.fusion_rate = 20.0  # Hz
        self.fusion_timer = self.create_timer(1.0/self.fusion_rate, self.fuse_data)
        
        self.get_logger().info('Sensor Fusion Node Initialized')

    def lidar_callback(self, msg):
        """Store LiDAR data"""
        self.lidar_data = msg

    def imu_callback(self, msg):
        """Store IMU data"""
        self.imu_data = msg

    def joint_callback(self, msg):
        """Store joint state data"""
        self.joint_data = msg

    def fuse_data(self):
        """Fuse sensor data from multiple sources"""
        if not all([self.lidar_data, self.imu_data, self.joint_data]):
            self.get_logger().debug('Waiting for sensor data...')
            return
        
        # Calculate fused metrics
        fused_data = Float32MultiArray()
        
        # From LiDAR: minimum distance, front obstacle detection
        min_distance = float(min([r for r in self.lidar_data.ranges if np.isfinite(r)], default=float('inf')))
        front_obstacle = self.detect_front_obstacle(self.lidar_data)
        
        # From IMU: orientation, stability metrics
        euler = self.quaternion_to_euler(
            self.imu_data.orientation.x,
            self.imu_data.orientation.y,
            self.imu_data.orientation.z,
            self.imu_data.orientation.w
        )
        orientation_data = [float(e) for e in euler]
        
        # From Joint States: joint position averages
        joint_positions_avg = np.mean([p for p in self.joint_data.position if not math.isnan(p)]) if self.joint_data.position else 0.0
        
        # Combine all data
        fused_data.data = [
            min_distance,           # 0: min distance from lidar
            float(front_obstacle),  # 1: front obstacle (true/false)
            *orientation_data,      # 2-4: roll, pitch, yaw
            joint_positions_avg,    # 5: average joint position
        ]
        
        # Publish fused data
        self.fused_data_publisher.publish(fused_data)
        
        # Estimate robot state
        self.estimate_robot_state(fused_data)

    def detect_front_obstacle(self, lidar_msg):
        """Detect if there is an obstacle in front"""
        # Check the front 30 degrees of the scan
        front_start_idx = int((len(lidar_msg.ranges) / 2) - (30 / 2) * len(lidar_msg.ranges) / (lidar_msg.angle_max - lidar_msg.angle_min) / math.pi * 180)
        front_end_idx = int((len(lidar_msg.ranges) / 2) + (30 / 2) * len(lidar_msg.ranges) / (lidar_msg.angle_max - lidar_msg.angle_min) / math.pi * 180)
        
        front_scan = lidar_msg.ranges[max(0, front_start_idx):min(len(lidar_msg.ranges), front_end_idx)]
        front_distances = [r for r in front_scan if np.isfinite(r)]
        
        if front_distances and min(front_distances) < 1.0:  # obstacle within 1 meter
            return 1.0
        else:
            return 0.0

    def quaternion_to_euler(self, x, y, z, w):
        """Convert quaternion to Euler angles"""
        sinr_cosp = 2 * (w * x + y * z)
        cosr_cosp = 1 - 2 * (x * x + y * y)
        roll = math.atan2(sinr_cosp, cosr_cosp)

        sinp = 2 * (w * y - z * x)
        if abs(sinp) >= 1:
            pitch = math.copysign(math.pi / 2, sinp)
        else:
            pitch = math.asin(sinp)

        siny_cosp = 2 * (w * z + x * y)
        cosy_cosp = 1 - 2 * (y * y + z * z)
        yaw = math.atan2(siny_cosp, cosy_cosp)

        return roll, pitch, yaw

    def estimate_robot_state(self, fused_data):
        """Estimate robot state based on fused sensor data"""
        try:
            odom_msg = Odometry()
            odom_msg.header.stamp = self.get_clock().now().to_msg()
            odom_msg.header.frame_id = 'odom'
            odom_msg.child_frame_id = 'base_link'
            
            # Estimate position based on movements (simplified)
            # In a real system, you would use odometry integration
            odom_msg.pose.pose.position.x = 0.0  # Placeholder
            odom_msg.pose.pose.position.y = 0.0  # Placeholder
            odom_msg.pose.pose.position.z = 0.0  # Placeholder
            
            # Use IMU orientation
            if self.imu_data:
                odom_msg.pose.pose.orientation = self.imu_data.orientation
            
            # Estimate velocity based on sensor changes
            # (This is a very simplified estimation)
            odom_msg.twist.twist.linear.x = 0.0  # Placeholder
            odom_msg.twist.twist.linear.y = 0.0  # Placeholder
            odom_msg.twist.twist.linear.z = 0.0  # Placeholder
            odom_msg.twist.twist.angular.z = 0.0  # Placeholder
            
            # Covariances (placeholder values)
            odom_msg.pose.covariance = [0.0] * 36
            odom_msg.twist.covariance = [0.0] * 36
            
            self.state_publisher.publish(odom_msg)
            
        except Exception as e:
            self.get_logger().error(f'Error estimating robot state: {str(e)}')

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info('Sensor Fusion Node Shutting Down')
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    fusion_node = SensorFusionNode()
    
    try:
        rclpy.spin(fusion_node)
    except KeyboardInterrupt:
        fusion_node.get_logger().info('Node interrupted by user')
    finally:
        fusion_node.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Testing Sensor Connections

### 1. Verification and Testing Script

Create a script to verify that sensors are properly connected:

```python
# scripts/test_sensor_connections.py
#!/usr/bin/env python3

"""
Test script to verify sensor connections between Gazebo and ROS 2.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan, Image, Imu, JointState
from std_msgs.msg import String
import threading
import time


class SensorConnectionTester(Node):
    def __init__(self):
        super().__init__('sensor_connection_tester')
        
        # Dictionary to track sensor status
        self.sensor_status = {
            'lidar': {'connected': False, 'last_msg_time': None, 'count': 0},
            'camera': {'connected': False, 'last_msg_time': None, 'count': 0},
            'imu': {'connected': False, 'last_msg_time': None, 'count': 0},
            'joint_states': {'connected': False, 'last_msg_time': None, 'count': 0}
        }
        
        # Create subscribers for all sensor types
        self.lidar_sub = self.create_subscription(
            LaserScan, '/laser/scan', self.lidar_callback, 10
        )
        
        self.camera_sub = self.create_subscription(
            Image, '/camera/image_raw', self.camera_callback, 10
        )
        
        self.imu_sub = self.create_subscription(
            Imu, '/imu/data', self.imu_callback, 10
        )
        
        self.joint_sub = self.create_subscription(
            JointState, '/joint_states', self.joint_callback, 10
        )
        
        # Publisher for test results
        self.test_results_pub = self.create_publisher(String, '/sensor_test_results', 10)
        
        # Timer to periodically report status
        self.report_timer = self.create_timer(5.0, self.report_status)
        
        self.get_logger().info('Sensor Connection Tester Initialized')

    def lidar_callback(self, msg):
        """Handle LiDAR message"""
        self.update_sensor_status('lidar')

    def camera_callback(self, msg):
        """Handle camera message"""
        self.update_sensor_status('camera')

    def imu_callback(self, msg):
        """Handle IMU message"""
        self.update_sensor_status('imu')

    def joint_callback(self, msg):
        """Handle joint state message"""
        self.update_sensor_status('joint_states')

    def update_sensor_status(self, sensor_type):
        """Update sensor status information"""
        current_time = self.get_clock().now()
        self.sensor_status[sensor_type]['connected'] = True
        self.sensor_status[sensor_type]['last_msg_time'] = current_time
        self.sensor_status[sensor_type]['count'] += 1

    def report_status(self):
        """Report sensor connection status"""
        report = "Sensor Connection Status:\n"
        
        all_connected = True
        for sensor, status in self.sensor_status.items():
            if status['connected'] and status['last_msg_time']:
                time_since_last = (self.get_clock().now() - status['last_msg_time']).nanoseconds / 1e9
                report += f"  {sensor}: CONNECTED (last msg: {time_since_last:.1f}s ago, {status['count']} msgs)\n"
                
                if time_since_last > 2.0:  # More than 2 seconds since last message
                    report += f"    WARNING: {sensor} messages delayed!\n"
                    all_connected = False
            else:
                report += f"  {sensor}: NOT CONNECTED\n"
                all_connected = False
        
        if all_connected:
            report += "\nAll sensors connected successfully!"
        else:
            report += "\nSome sensors are not properly connected."
        
        self.get_logger().info(report)
        
        # Publish results
        results_msg = String()
        results_msg.data = report
        self.test_results_pub.publish(results_msg)

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info('Sensor Connection Tester Shutting Down')
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    tester = SensorConnectionTester()
    
    try:
        # Run for 30 seconds then exit
        start_time = time.time()
        while time.time() - start_time < 30 and rclpy.ok():
            rclpy.spin_once(tester, timeout_sec=0.1)
        
        tester.get_logger().info('Test completed')
        
    except KeyboardInterrupt:
        tester.get_logger().info('Test interrupted by user')
    finally:
        tester.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Launch File for Sensor Processing

### 1. Complete Sensor Processing Launch

Create a launch file to start all sensor processing nodes:

```python
# launch/sensor_processing.launch.py
from launch import LaunchDescription
from launch.actions import DeclareLaunchArgument, IncludeLaunchDescription, TimerAction
from launch.launch_description_sources import PythonLaunchDescriptionSource
from launch.substitutions import LaunchConfiguration, PathJoinSubstitution
from launch_ros.actions import Node, SetParameter
from launch_ros.substitutions import FindPackageShare
from ament_index_python.packages import get_package_share_directory


def generate_launch_description():
    # Declare launch arguments
    use_sim_time = LaunchConfiguration('use_sim_time', default='true')
    
    # Gazebo launch (if needed)
    gazebo = IncludeLaunchDescription(
        PythonLaunchDescriptionSource([
            get_package_share_directory('gazebo_ros'),
            '/launch/empty_world.launch.py'
        ]),
    )
    
    # Robot State Publisher (for transforms)
    robot_state_publisher = Node(
        package='robot_state_publisher',
        executable='robot_state_publisher',
        name='robot_state_publisher',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # LiDAR processing node
    lidar_processor = Node(
        package='humanoid_description',  # Adjust to your package name
        executable='lidar_processor',
        name='lidar_processor',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # Camera processing node
    camera_processor = Node(
        package='humanoid_description',  # Adjust to your package name
        executable='camera_processor',
        name='camera_processor',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # IMU processing node
    imu_processor = Node(
        package='humanoid_description',  # Adjust to your package name
        executable='imu_processor',
        name='imu_processor',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # Sensor fusion node
    sensor_fusion = Node(
        package='humanoid_description',  # Adjust to your package name
        executable='sensor_fusion_node',
        name='sensor_fusion_node',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # Sensor connection tester (for verification)
    sensor_tester = Node(
        package='humanoid_description',  # Adjust to your package name
        executable='test_sensor_connections',
        name='sensor_connection_tester',
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    # RViz for visualization
    rviz_config = PathJoinSubstitution([
        FindPackageShare('humanoid_description'),
        'rviz',
        'sensors.rviz'
    ])
    
    rviz = Node(
        package='rviz2',
        executable='rviz2',
        name='rviz2',
        arguments=['-d', rviz_config],
        parameters=[{'use_sim_time': use_sim_time}],
        output='screen'
    )
    
    return LaunchDescription([
        SetParameter(name='use_sim_time', value=use_sim_time),
        
        # Launch Gazebo
        gazebo,
        
        # Launch robot state publisher
        robot_state_publisher,
        
        # Launch sensor processing nodes with delays to ensure proper initialization
        TimerAction(
            period=2.0,
            actions=[lidar_processor]
        ),
        
        TimerAction(
            period=3.0,
            actions=[camera_processor]
        ),
        
        TimerAction(
            period=3.0,
            actions=[imu_processor]
        ),
        
        TimerAction(
            period=4.0,
            actions=[sensor_fusion]
        ),
        
        TimerAction(
            period=5.0,
            actions=[sensor_tester]
        ),
        
        TimerAction(
            period=6.0,
            actions=[rviz]
        ),
    ])
```

## Performance Optimization

### 1. Efficient Message Handling

Optimize your nodes for efficient sensor data processing:

```python
# performance_optimized_processor.py
#!/usr/bin/env python3

"""
Performance-optimized sensor processor with efficient data handling.
"""

import rclpy
from rclpy.node import Node
from sensor_msgs.msg import LaserScan
from std_msgs.msg import Header
import numpy as np
from collections import deque
import time


class EfficientSensorProcessor(Node):
    def __init__(self):
        super().__init__('efficient_sensor_processor')
        
        # Use a limited queue to prevent memory issues with high-frequency data
        self.scan_buffer = deque(maxlen=5)  # Only keep last 5 scans
        
        # Subscribe to LiDAR data with custom QoS for performance
        from rclpy.qos import QoSProfile, QoSDurabilityPolicy, QoSHistoryPolicy, QoSReliabilityPolicy
        
        qos_profile = QoSProfile(
            depth=10,
            durability=QoSDurabilityPolicy.VOLATILE,
            history=QoSHistoryPolicy.KEEP_LAST,
            reliability=QoSReliabilityPolicy.BEST_EFFORT
        )
        
        self.lidar_subscription = self.create_subscription(
            LaserScan,
            '/laser/scan',
            self.lidar_callback,
            qos_profile
        )
        
        # Publisher with same QoS settings
        self.processed_publisher = self.create_publisher(
            LaserScan,
            '/processed_lidar',
            qos_profile
        )
        
        # Throttle processing rate
        self.processing_rate = 10  # Hz
        self.last_process_time = time.time()
        
        # Statistics
        self.message_count = 0
        self.processed_count = 0
        
        self.get_logger().info('Efficient Sensor Processor Initialized')

    def lidar_callback(self, msg):
        """Efficiently handle LiDAR data"""
        self.message_count += 1
        self.scan_buffer.append((msg, time.time()))
        
        # Process at throttled rate
        current_time = time.time()
        if (current_time - self.last_process_time) >= (1.0 / self.processing_rate):
            self.process_buffered_scans()
            self.last_process_time = current_time

    def process_buffered_scans(self):
        """Process all buffered scans efficiently"""
        if not self.scan_buffer:
            return
            
        # Process the most recent scan
        latest_msg, timestamp = self.scan_buffer[-1]
        
        try:
            # Convert to numpy for efficient processing
            ranges_array = np.array(latest_msg.ranges, dtype=np.float32)
            
            # Perform efficient filtering operations
            valid_mask = np.isfinite(ranges_array)
            valid_ranges = ranges_array[valid_mask]
            
            if len(valid_ranges) > 0:
                # Calculate statistics efficiently using numpy
                min_range = np.min(valid_ranges)
                max_range = np.max(valid_ranges)
                mean_range = np.mean(valid_ranges)
                
                # Create a processed message
                processed_msg = LaserScan()
                processed_msg.header = Header()
                processed_msg.header.stamp = self.get_clock().now().to_msg()
                processed_msg.header.frame_id = latest_msg.header.frame_id
                
                # Copy metadata
                processed_msg.angle_min = latest_msg.angle_min
                processed_msg.angle_max = latest_msg.angle_max
                processed_msg.angle_increment = latest_msg.angle_increment
                processed_msg.time_increment = latest_msg.time_increment
                processed_msg.scan_time = latest_msg.scan_time
                processed_msg.range_min = latest_msg.range_min
                processed_msg.range_max = latest_msg.range_max
                
                # Use processed ranges
                processed_msg.ranges = ranges_array.tolist()  # Convert back if needed
                
                self.processed_publisher.publish(processed_msg)
                self.processed_count += 1
                
        except Exception as e:
            self.get_logger().error(f'Error processing scan: {str(e)}')

    def destroy_node(self):
        """Cleanup before node destruction"""
        self.get_logger().info(
            f'Efficient Sensor Processor Shutting Down - '
            f'Received: {self.message_count}, Processed: {self.processed_count}'
        )
        super().destroy_node()


def main(args=None):
    rclpy.init(args=args)
    
    processor = EfficientSensorProcessor()
    
    try:
        rclpy.spin(processor)
    except KeyboardInterrupt:
        processor.get_logger().info('Node interrupted by user')
    finally:
        processor.destroy_node()
        rclpy.shutdown()


if __name__ == '__main__':
    main()
```

## Troubleshooting Common Issues

### 1. Sensor Connection Problems

**Common Issues and Solutions:**

1. **No data received from sensors:**
   - Check that Gazebo plugins are properly configured in URDF
   - Verify topic names match between Gazebo output and ROS subscription
   - Check ROS_DOMAIN_ID if using multiple machines

2. **High latency in sensor data:**
   - Reduce sensor update rates in Gazebo configuration
   - Use QoS settings optimized for performance
   - Check system resources (CPU, memory)

3. **Incorrect frame transformations:**
   - Verify TF tree is properly published
   - Check that sensor frames are correctly defined in URDF
   - Use `ros2 run tf2_tools view_frames` to visualize the transform tree

4. **Memory issues with high-frequency sensors:**
   - Implement message buffering with size limits
   - Use efficient data structures (numpy arrays)
   - Throttle processing rates appropriately

## Next Steps

With Gazebo sensor data properly connected to ROS 2 nodes, you'll next connect Unity visualization to ROS 2 nodes. This will complete the full simulation pipeline where sensors are captured in Gazebo, processed in ROS 2, and visualized in Unity.

The sensor connection framework you've implemented provides the foundation for all perception capabilities in your humanoid robot system.